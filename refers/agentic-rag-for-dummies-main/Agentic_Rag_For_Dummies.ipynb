{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovanniPasq/agentic-rag-for-dummies/blob/main/Agentic_Rag_For_Dummies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "321dbfd3",
      "metadata": {
        "id": "321dbfd3"
      },
      "outputs": [],
      "source": [
        "#run this cell only in colab, otherwise create a venv and install requirements.txt available in the project folder\n",
        "!pip install --quiet --upgrade langgraph\n",
        "!pip install -qU langchain-ollama\n",
        "!pip install -qU langchain langchain-community langchain-qdrant langchain-huggingface qdrant-client fastembed flashrank langchain-core\n",
        "!pip install --upgrade gradio\n",
        "\n",
        "# Optional (example): if you want to use Gemini models\n",
        "#!pip install -qU \"langchain[google-genai]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9782958",
      "metadata": {
        "id": "c9782958"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOCK 1: Configuration and Environment Setup\n",
        "# ============================================================\n",
        "import os\n",
        "\n",
        "# Configuration\n",
        "DOCS_DIR = \"docs\"  # Directory containing your pdfs files\n",
        "MARKDOWN_DIR = \"markdown\" # Directory containing the pdfs converted to markdown\n",
        "PARENT_STORE_PATH = \"parent_store\"  # Directory for parent chunk JSON files\n",
        "CHILD_COLLECTION = \"document_child_chunks\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(DOCS_DIR, exist_ok=True)\n",
        "os.makedirs(MARKDOWN_DIR, exist_ok=True)\n",
        "os.makedirs(PARENT_STORE_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e8989b7",
      "metadata": {
        "id": "7e8989b7"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOCK 2: LLM Initialization\n",
        "# ============================================================\n",
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatOllama(model=\"qwen3:4b-instruct-2507-q4_K_M\", temperature=0)\n",
        "\n",
        "# Alternative (example): Google Gemini\n",
        "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"your-api-key-here\"\n",
        "# llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f982c53",
      "metadata": {
        "id": "9f982c53"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOCK 3: Embeddings Setup\n",
        "# ============================================================\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_qdrant.fastembed_sparse import FastEmbedSparse\n",
        "\n",
        "# Dense embeddings for semantic understanding\n",
        "dense_embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
        ")\n",
        "\n",
        "# Sparse embeddings for keyword matching\n",
        "sparse_embeddings = FastEmbedSparse(\n",
        "    model_name=\"Qdrant/bm25\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11efc63b",
      "metadata": {
        "id": "11efc63b"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOCK 4: Vector Database Setup\n",
        "# ============================================================\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models as qmodels\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from langchain_qdrant.qdrant import RetrievalMode\n",
        "\n",
        "# Initialize Qdrant client (local file-based storage)\n",
        "client = QdrantClient(path=\"qdrant_db\")\n",
        "\n",
        "# Get embedding dimension\n",
        "embedding_dimension = len(dense_embeddings.embed_query(\"test\"))\n",
        "\n",
        "def ensure_collection(collection_name):\n",
        "    \"\"\"Create Qdrant collection if it doesn't exist\"\"\"\n",
        "    if not client.collection_exists(collection_name):\n",
        "        client.create_collection(\n",
        "            collection_name=collection_name,\n",
        "            vectors_config=qmodels.VectorParams(\n",
        "                size=embedding_dimension,\n",
        "                distance=qmodels.Distance.COSINE\n",
        "            ),\n",
        "            sparse_vectors_config={\n",
        "                \"sparse\": qmodels.SparseVectorParams()\n",
        "            },\n",
        "        )\n",
        "        print(f\"âœ“ Created collection: {collection_name}\")\n",
        "    else:\n",
        "        print(f\"âœ“ Collection already exists: {collection_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06fd6518",
      "metadata": {
        "id": "06fd6518"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOCK 5: PDFs Conversion to Markdown\n",
        "# ============================================================\n",
        "\n",
        "# For more details on converting PDFs to Markdown, refer to the `pdf_to_md.ipynb` notebook available in the repository.\n",
        "import os\n",
        "import pymupdf.layout\n",
        "import pymupdf4llm\n",
        "from pathlib import Path\n",
        "import glob\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "def pdf_to_markdown(pdf_path, output_dir):\n",
        "    doc = pymupdf.open(pdf_path)\n",
        "    md = pymupdf4llm.to_markdown(doc, header=False, footer=False, page_separators=True, ignore_images=True, write_images=False, image_path=None)\n",
        "    md_cleaned = md.encode('utf-8', errors='surrogatepass').decode('utf-8', errors='ignore')\n",
        "    output_path = Path(output_dir) / Path(doc.name).stem\n",
        "    Path(output_path).with_suffix(\".md\").write_bytes(md_cleaned.encode('utf-8'))\n",
        "\n",
        "def pdfs_to_markdowns(path_pattern, overwrite: bool = False):\n",
        "    output_dir = Path(MARKDOWN_DIR)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for pdf_path in map(Path, glob.glob(path_pattern)):\n",
        "        md_path = (output_dir / pdf_path.stem).with_suffix(\".md\")\n",
        "        if overwrite or not md_path.exists():\n",
        "            pdf_to_markdown(pdf_path, output_dir)\n",
        "\n",
        "pdfs_to_markdowns(f\"{DOCS_DIR}/*.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e103d352",
      "metadata": {
        "id": "e103d352",
        "outputId": "d22ee97b-01c7-4cee-c4c0-d27488a94f03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Created collection: document_child_chunks\n",
            "ðŸ“„ Processing: blockchain.md\n",
            "ðŸ“„ Processing: fortinet.md\n",
            "ðŸ“„ Processing: javascript_tutorial.md\n",
            "\n",
            "ðŸ” Indexing 1036 child chunks into Qdrant...\n",
            "âœ“ Child chunks indexed successfully\n",
            "ðŸ’¾ Saving 81 parent chunks to JSON...\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# BLOCK 6: Document Indexing\n",
        "# ============================================================\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "from pathlib import Path\n",
        "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
        "\n",
        "if client.collection_exists(CHILD_COLLECTION):\n",
        "    print(f\"Removing existing Qdrant collection: {CHILD_COLLECTION}\")\n",
        "    client.delete_collection(CHILD_COLLECTION)\n",
        "    ensure_collection(CHILD_COLLECTION)\n",
        "else:\n",
        "    ensure_collection(CHILD_COLLECTION)\n",
        "\n",
        "child_vector_store = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=CHILD_COLLECTION,\n",
        "    embedding=dense_embeddings,\n",
        "    sparse_embedding=sparse_embeddings,\n",
        "    retrieval_mode=RetrievalMode.HYBRID,\n",
        "    sparse_vector_name=\"sparse\"\n",
        ")\n",
        "\n",
        "def index_documents():\n",
        "    headers_to_split_on = [(\"#\", \"H1\"), (\"##\", \"H2\"), (\"###\", \"H3\")]\n",
        "    parent_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
        "    child_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "\n",
        "    min_parent_size = 2000\n",
        "    max_parent_size = 10000\n",
        "\n",
        "    all_parent_pairs, all_child_chunks = [], []\n",
        "    md_files = sorted(glob.glob(os.path.join(MARKDOWN_DIR, \"*.md\")))\n",
        "\n",
        "    if not md_files:\n",
        "        print(f\"âš ï¸  No .md files found in {MARKDOWN_DIR}/\")\n",
        "        return\n",
        "\n",
        "    for doc_path_str in md_files:\n",
        "        doc_path = Path(doc_path_str)\n",
        "        print(f\"ðŸ“„ Processing: {doc_path.name}\")\n",
        "\n",
        "        try:\n",
        "            with open(doc_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                md_text = f.read()\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error reading {doc_path.name}: {e}\")\n",
        "            continue\n",
        "\n",
        "        parent_chunks = parent_splitter.split_text(md_text)\n",
        "        merged_parents = merge_small_parents(parent_chunks, min_parent_size)\n",
        "        split_parents = split_large_parents(merged_parents, max_parent_size, child_splitter)\n",
        "        cleaned_parents = clean_small_chunks(split_parents, min_parent_size)\n",
        "\n",
        "        for i, p_chunk in enumerate(cleaned_parents):\n",
        "            parent_id = f\"{doc_path.stem}_parent_{i}\"\n",
        "            p_chunk.metadata.update({\"source\": doc_path.stem + \".pdf\", \"parent_id\": parent_id})\n",
        "            all_parent_pairs.append((parent_id, p_chunk))\n",
        "            children = child_splitter.split_documents([p_chunk])\n",
        "            all_child_chunks.extend(children)\n",
        "\n",
        "    if not all_child_chunks:\n",
        "        print(\"âš ï¸ No child chunks to index\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nðŸ” Indexing {len(all_child_chunks)} child chunks into Qdrant...\")\n",
        "    try:\n",
        "        child_vector_store.add_documents(all_child_chunks)\n",
        "        print(\"âœ“ Child chunks indexed successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error indexing child chunks: {e}\")\n",
        "        return\n",
        "\n",
        "    print(f\"ðŸ’¾ Saving {len(all_parent_pairs)} parent chunks to JSON...\")\n",
        "    for item in os.listdir(PARENT_STORE_PATH):\n",
        "        os.remove(os.path.join(PARENT_STORE_PATH, item))\n",
        "\n",
        "    for parent_id, doc in all_parent_pairs:\n",
        "        doc_dict = {\"page_content\": doc.page_content, \"metadata\": doc.metadata}\n",
        "        filepath = os.path.join(PARENT_STORE_PATH, f\"{parent_id}.json\")\n",
        "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(doc_dict, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def merge_small_parents(chunks, min_size):\n",
        "    if not chunks:\n",
        "        return []\n",
        "\n",
        "    merged, current = [], None\n",
        "\n",
        "    for chunk in chunks:\n",
        "        if current is None:\n",
        "            current = chunk\n",
        "        else:\n",
        "            current.page_content += \"\\n\\n\" + chunk.page_content\n",
        "            for k, v in chunk.metadata.items():\n",
        "                if k in current.metadata:\n",
        "                    current.metadata[k] = f\"{current.metadata[k]} -> {v}\"\n",
        "                else:\n",
        "                    current.metadata[k] = v\n",
        "\n",
        "        if len(current.page_content) >= min_size:\n",
        "            merged.append(current)\n",
        "            current = None\n",
        "\n",
        "    if current:\n",
        "        if merged:\n",
        "            merged[-1].page_content += \"\\n\\n\" + current.page_content\n",
        "            for k, v in current.metadata.items():\n",
        "                if k in merged[-1].metadata:\n",
        "                    merged[-1].metadata[k] = f\"{merged[-1].metadata[k]} -> {v}\"\n",
        "                else:\n",
        "                    merged[-1].metadata[k] = v\n",
        "        else:\n",
        "            merged.append(current)\n",
        "\n",
        "    return merged\n",
        "\n",
        "def split_large_parents(chunks, max_size, splitter):\n",
        "    split_chunks = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        if len(chunk.page_content) <= max_size:\n",
        "            split_chunks.append(chunk)\n",
        "        else:\n",
        "            large_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=max_size,\n",
        "                chunk_overlap=splitter._chunk_overlap\n",
        "            )\n",
        "            sub_chunks = large_splitter.split_documents([chunk])\n",
        "            split_chunks.extend(sub_chunks)\n",
        "\n",
        "    return split_chunks\n",
        "\n",
        "def clean_small_chunks(chunks, min_size):\n",
        "    cleaned = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        if len(chunk.page_content) < min_size:\n",
        "            if cleaned:\n",
        "                cleaned[-1].page_content += \"\\n\\n\" + chunk.page_content\n",
        "                for k, v in chunk.metadata.items():\n",
        "                    if k in cleaned[-1].metadata:\n",
        "                        cleaned[-1].metadata[k] = f\"{cleaned[-1].metadata[k]} -> {v}\"\n",
        "                    else:\n",
        "                        cleaned[-1].metadata[k] = v\n",
        "            elif i < len(chunks) - 1:\n",
        "                chunks[i + 1].page_content = chunk.page_content + \"\\n\\n\" + chunks[i + 1].page_content\n",
        "                for k, v in chunk.metadata.items():\n",
        "                    if k in chunks[i + 1].metadata:\n",
        "                        chunks[i + 1].metadata[k] = f\"{v} -> {chunks[i + 1].metadata[k]}\"\n",
        "                    else:\n",
        "                        chunks[i + 1].metadata[k] = v\n",
        "            else:\n",
        "                cleaned.append(chunk)\n",
        "        else:\n",
        "            cleaned.append(chunk)\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "index_documents()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8eac029",
      "metadata": {
        "id": "b8eac029"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOCK 7: Tool Definitions\n",
        "# ============================================================\n",
        "import json\n",
        "from typing import List\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def search_child_chunks(query: str, k: int = 5) -> List[dict]:\n",
        "    \"\"\"Search for the top K most relevant child chunks.\n",
        "\n",
        "    Args:\n",
        "        query: Search query string\n",
        "        k: Number of results to return\n",
        "    \"\"\"\n",
        "    try:\n",
        "        results = child_vector_store.similarity_search(query, k=k, score_threshold=0.7)\n",
        "        return [\n",
        "            {\n",
        "                \"content\": doc.page_content,\n",
        "                \"parent_id\": doc.metadata.get(\"parent_id\", \"\"),\n",
        "                \"source\": doc.metadata.get(\"source\", \"\")\n",
        "            }\n",
        "            for doc in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"Error searching child chunks: {e}\")\n",
        "        return []\n",
        "\n",
        "@tool\n",
        "def retrieve_parent_chunks(parent_ids: List[str]) -> List[dict]:\n",
        "    \"\"\"Retrieve full parent chunks by their IDs.\n",
        "\n",
        "    Args:\n",
        "        parent_ids: List of parent chunk IDs to retrieve\n",
        "    \"\"\"\n",
        "    unique_ids = sorted(list(set(parent_ids)))\n",
        "    results = []\n",
        "\n",
        "    for parent_id in unique_ids:\n",
        "        file_path = os.path.join(PARENT_STORE_PATH, parent_id if parent_id.lower().endswith(\".json\") else f\"{parent_id}.json\")\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    doc_dict = json.load(f)\n",
        "                    results.append({\n",
        "                        \"content\": doc_dict[\"page_content\"],\n",
        "                        \"parent_id\": parent_id,\n",
        "                        \"metadata\": doc_dict[\"metadata\"]\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading parent chunk {parent_id}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Bind tools to LLM\n",
        "llm_with_tools = llm.bind_tools([search_child_chunks, retrieve_parent_chunks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b12809c6",
      "metadata": {
        "id": "b12809c6"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOCK 8: System Prompts\n",
        "# ============================================================\n",
        "def get_conversation_summary_prompt() -> str:\n",
        "    return \"\"\"\n",
        "        Summarize the key topics and context from this conversation in 1-2 concise sentences.\n",
        "\n",
        "        Focus on:\n",
        "        - Main topics discussed\n",
        "        - Important facts or entities mentioned\n",
        "        - Any unresolved questions\n",
        "\n",
        "        Discard: greetings, misunderstandings, off-topic content.\n",
        "        If no meaningful topics exist, return an empty string.\n",
        "\n",
        "        Output:\n",
        "\n",
        "        - Return ONLY the summary.\n",
        "        - Do NOT include any explanations or justifications.\n",
        "        \"\"\"\n",
        "\n",
        "def get_query_analysis_prompt() -> str:\n",
        "    return \"\"\"\n",
        "        Rewrite the user query so it can be used for document retrieval.\n",
        "\n",
        "        Rules:\n",
        "\n",
        "        - The final query must be clear and self-contained.\n",
        "        - Always return at least one rewritten query.\n",
        "        - If the query contains a specific product name, brand, proper noun, or technical term,\n",
        "        treat it as domain-specific and IGNORE the conversation context.\n",
        "        - Use the conversation context ONLY if it is needed to understand the query\n",
        "        OR to determine the domain when the query itself is ambiguous.\n",
        "        - If the query is clear but underspecified, use relevant context to disambiguate.\n",
        "        - Do NOT use context to reinterpret or replace explicit terms in the query.\n",
        "        - Do NOT add new constraints, subtopics, or details not explicitly asked.\n",
        "        - Fix grammar, typos, and unclear abbreviations.\n",
        "        - Remove filler words and conversational wording.\n",
        "        - Use concrete keywords and entities ONLY if already implied.\n",
        "\n",
        "        Splitting:\n",
        "        - If the query contains multiple unrelated information needs,\n",
        "        split it into at most 3 separate search queries.\n",
        "        - When splitting, keep each sub-query semantically equivalent.\n",
        "        - Do NOT enrich or expand meaning.\n",
        "        - Do NOT split unless it improves retrieval.\n",
        "\n",
        "        Failure:\n",
        "        - If the intent is unclear or meaningless, mark as unclear.\n",
        "        \"\"\"\n",
        "\n",
        "def get_rag_agent_system_prompt() -> str:\n",
        "    return \"\"\"\n",
        "        You are a retrieval-augmented assistant.\n",
        "\n",
        "        You are NOT allowed to answer immediately.\n",
        "\n",
        "        Before producing ANY final answer, you must first perform a document search\n",
        "        and observe retrieved content.\n",
        "\n",
        "        If you have not searched, the answer is invalid.\n",
        "\n",
        "        Workflow:\n",
        "        1. Search the documents using the user query.\n",
        "        2. Inspect retrieved excerpts and keep only relevant ones.\n",
        "        3. Retrieve additional surrounding context ONLY if excerpts are insufficient.\n",
        "        4. Stop retrieval as soon as information is sufficient.\n",
        "        5. Answer using ONLY retrieved information.\n",
        "        6. List file name at the end.\n",
        "\n",
        "        Retry rule:\n",
        "        - If no relevant information is found, rewrite the query into a concise,\n",
        "        answer-focused statement and restart the process from STEP 1.\n",
        "        - Perform this retry only once.\n",
        "\n",
        "        If no relevant information is found after the retry, say so.\n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4c066b1",
      "metadata": {
        "id": "c4c066b1"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOCK 9: State Definitions\n",
        "# ============================================================\n",
        "from langgraph.graph import MessagesState\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "class State(MessagesState):\n",
        "    questionIsClear: bool\n",
        "    conversation_summary: str = \"\"\n",
        "\n",
        "class QueryAnalysis(BaseModel):\n",
        "    is_clear: bool = Field(\n",
        "        description=\"Indicates if the user's question is clear and answerable.\"\n",
        "    )\n",
        "    questions: List[str] = Field(\n",
        "        description=\"List of rewritten, self-contained questions.\"\n",
        "    )\n",
        "    clarification_needed: str = Field(\n",
        "        description=\"Explanation if the question is unclear.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "948ca8ee",
      "metadata": {
        "id": "948ca8ee"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOCK 10: Graph Node Functions\n",
        "# ============================================================\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, RemoveMessage\n",
        "from typing import Literal\n",
        "\n",
        "def analyze_chat_and_summarize(state: State):\n",
        "    \"\"\"\n",
        "    Analyzes chat history and summarizes key points for context.\n",
        "    \"\"\"\n",
        "    if len(state[\"messages\"]) < 4:  # Need some history to summarize\n",
        "        return {\"conversation_summary\": \"\"}\n",
        "\n",
        "    # Extract relevant messages (excluding current query and system messages)\n",
        "    relevant_msgs = [\n",
        "        msg for msg in state[\"messages\"][:-1]  # Exclude current query\n",
        "        if isinstance(msg, (HumanMessage, AIMessage))\n",
        "        and not getattr(msg, \"tool_calls\", None)\n",
        "    ]\n",
        "\n",
        "    if not relevant_msgs:\n",
        "        return {\"conversation_summary\": \"\"}\n",
        "\n",
        "    conversation = \"Conversation history:\\n\"\n",
        "    for msg in relevant_msgs[-6:]:\n",
        "        role = \"User\" if isinstance(msg, HumanMessage) else \"Assistant\"\n",
        "        conversation += f\"{role}: {msg.content}\\n\"\n",
        "\n",
        "    summary_response = llm.with_config(temperature=0.2).invoke([SystemMessage(content=get_conversation_summary_prompt())] + [HumanMessage(content=conversation)])\n",
        "    return {\"conversation_summary\": summary_response.content}\n",
        "\n",
        "def analyze_and_rewrite_query(state: State):\n",
        "    \"\"\"\n",
        "    Analyzes user query and rewrites it for clarity, optionally using conversation context.\n",
        "    \"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    conversation_summary = state.get(\"conversation_summary\", \"\")\n",
        "\n",
        "    context_section = (f\"Conversation Context:\\n{conversation_summary}\\n\" if conversation_summary.strip() else \"\") + f\"User Query:\\n{last_message.content}\\n\"\n",
        "\n",
        "    llm_with_structure = llm.with_config(temperature=0.1).with_structured_output(QueryAnalysis)\n",
        "    response = llm_with_structure.invoke([SystemMessage(content=get_query_analysis_prompt())] + [HumanMessage(content=context_section)])\n",
        "\n",
        "    if response.is_clear:\n",
        "        # Remove all non-system messages\n",
        "        delete_all = [\n",
        "            RemoveMessage(id=m.id)\n",
        "            for m in state[\"messages\"]\n",
        "            if not isinstance(m, SystemMessage)\n",
        "        ]\n",
        "\n",
        "        rewritten = (\n",
        "            \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(response.questions)])\n",
        "            if len(response.questions) > 1\n",
        "            else response.questions[0]\n",
        "        )\n",
        "        return {\n",
        "            \"questionIsClear\": True,\n",
        "            \"messages\": delete_all + [HumanMessage(content=rewritten)]\n",
        "        }\n",
        "    else:\n",
        "        clarification = response.clarification_needed or \"I need more information to understand your question.\"\n",
        "        return {\n",
        "            \"questionIsClear\": False,\n",
        "            \"messages\": [AIMessage(content=clarification)]\n",
        "        }\n",
        "\n",
        "def human_input_node(state: State):\n",
        "    \"\"\"Placeholder node for human-in-the-loop interruption\"\"\"\n",
        "    return {}\n",
        "\n",
        "def route_after_rewrite(state: State) -> Literal[\"agent\", \"human_input\"]:\n",
        "    \"\"\"Route to agent if question is clear, otherwise wait for human input\"\"\"\n",
        "    return \"agent\" if state.get(\"questionIsClear\", False) else \"human_input\"\n",
        "\n",
        "def agent_node(state: State):\n",
        "    \"\"\"Main agent node that processes queries using tools\"\"\"\n",
        "    response = llm_with_tools.invoke([SystemMessage(content=get_rag_agent_system_prompt())] + state[\"messages\"])\n",
        "    return {\"messages\": [response]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8c111af",
      "metadata": {
        "id": "d8c111af",
        "outputId": "17a46180-a492-45a5-88cf-7e76450670f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Agent graph compiled successfully.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# BLOCK 11: Graph Construction\n",
        "# ============================================================\n",
        "from langgraph.graph import START, StateGraph\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "# Initialize checkpointer\n",
        "checkpointer = InMemorySaver()\n",
        "\n",
        "# Create graph builder\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# Add nodes\n",
        "graph_builder.add_node(\"summarize\", analyze_chat_and_summarize)\n",
        "graph_builder.add_node(\"analyze_rewrite\", analyze_and_rewrite_query)\n",
        "graph_builder.add_node(\"human_input\", human_input_node)\n",
        "graph_builder.add_node(\"agent\", agent_node)\n",
        "graph_builder.add_node(\"tools\", ToolNode([search_child_chunks, retrieve_parent_chunks]))\n",
        "\n",
        "# Add edges\n",
        "graph_builder.add_edge(START, \"summarize\")\n",
        "graph_builder.add_edge(\"summarize\", \"analyze_rewrite\")\n",
        "graph_builder.add_conditional_edges(\"analyze_rewrite\", route_after_rewrite)\n",
        "graph_builder.add_edge(\"human_input\", \"analyze_rewrite\")\n",
        "graph_builder.add_conditional_edges(\"agent\", tools_condition)\n",
        "graph_builder.add_edge(\"tools\", \"agent\")\n",
        "\n",
        "# Compile graph\n",
        "agent_graph = graph_builder.compile(\n",
        "    checkpointer=checkpointer,\n",
        "    interrupt_before=[\"human_input\"]\n",
        ")\n",
        "\n",
        "print(\"âœ“ Agent graph compiled successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aadf1ed",
      "metadata": {
        "id": "5aadf1ed"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOCK 12: Gradio Interface\n",
        "# ============================================================\n",
        "\n",
        "#For a complete end-to-end pipeline Gradio interface, including document ingestion, please refer to the project folder\n",
        "\n",
        "import gradio as gr\n",
        "import uuid\n",
        "\n",
        "def create_thread_id():\n",
        "    \"\"\"Generate a unique thread ID for each conversation\"\"\"\n",
        "    return {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
        "\n",
        "def clear_session():\n",
        "    \"\"\"Clear thread for new conversation and clean up checkpointer state\"\"\"\n",
        "    global config\n",
        "    agent_graph.checkpointer.delete_thread(config[\"configurable\"][\"thread_id\"])\n",
        "    config = create_thread_id()\n",
        "\n",
        "def chat_with_agent(message, history):\n",
        "    \"\"\"\n",
        "    Handle chat with human-in-the-loop support.\n",
        "    Returns: response text\n",
        "    \"\"\"\n",
        "    current_state = agent_graph.get_state(config)\n",
        "    if current_state.next:\n",
        "        agent_graph.update_state(config,{\"messages\": [HumanMessage(content=message.strip())]})\n",
        "        result = agent_graph.invoke(None, config)\n",
        "    else:\n",
        "        result = agent_graph.invoke({\"messages\": [HumanMessage(content=message.strip())]}, config)\n",
        "    return result['messages'][-1].content\n",
        "\n",
        "# Initialize thread configuration\n",
        "config = create_thread_id()\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(height=600, placeholder=\"<strong>Ask me anything!</strong><br><em>I'll search, reason, and act to give you the best answer :)</em>\")\n",
        "    chatbot.clear(clear_session)\n",
        "    gr.ChatInterface(fn=chat_with_agent, chatbot=chatbot)\n",
        "\n",
        "print(\"\\nLaunching application...\")\n",
        "demo.launch(theme=gr.themes.Citrus())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}